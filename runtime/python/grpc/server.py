# æ ‡å‡† Python å’Œ gRPC ç›¸å…³çš„ä¾èµ–
import os
import sys
from concurrent import futures
import argparse
import cosyvoice_pb2
import cosyvoice_pb2_grpc
import logging
import torch
import grpc
import torchaudio
import numpy as np

# ROOT_DIR = ... å’Œ sys.path.append ç”¨äºæŠŠé¡¹ç›®ä¸Šçš„æœ¬åœ°åŒ…ï¼ˆåŒ…æ‹¬ cosyvoice ä»¥åŠ Matcha-TTS ç­‰ï¼‰åŠ å…¥ Python Pathï¼Œç¡®ä¿æœ¬åœ°æ¨¡å—å¯¼å…¥æ—¶èƒ½æ­£å¸¸æ‰¾åˆ°ã€‚
# from cosyvoice.cli.cosyvoice import CosyVoice2 è¡¨æ˜æˆ‘ä»¬è¦ä½¿ç”¨ CosyVoice2 è¿™ä¸ªç±»ï¼ˆè€Œä¸æ˜¯å®˜æ–¹ç¤ºä¾‹ä¸­çš„ CosyVoice ç±»ï¼‰ã€‚
ROOT_DIR = os.path.dirname(os.path.abspath(__file__))
sys.path.append('{}/../../..'.format(ROOT_DIR))
sys.path.append('{}/../../../third_party/Matcha-TTS'.format(ROOT_DIR))
from cosyvoice.cli.cosyvoice import CosyVoice2

# è®¾ç½®æ—¥å¿—çº§åˆ«ä¸º DEBUGï¼Œæ–¹ä¾¿è°ƒè¯•æ—¶è¾“å‡ºæ›´å¤šçš„ä¿¡æ¯ã€‚
# è®¾ç½®æ—¥å¿—æ ¼å¼ï¼Œä»¥ä¾¿æ›´å¥½åœ°æŸ¥çœ‹æ—¶é—´ã€æ—¥å¿—çº§åˆ«ã€å…·ä½“æ—¥å¿—å†…å®¹ã€‚
logging.basicConfig(level=logging.DEBUG,
                    format='%(asctime)s %(levelname)s %(message)s')

# è¿™é‡Œ CosyVoiceServiceImpl ç»§æ‰¿äº† cosyvoice_pb2_grpc.CosyVoiceServicerï¼Œè¿™ä¹Ÿæ˜¯å®˜æ–¹ç¤ºä¾‹çš„æ€è·¯ï¼šå®ç° gRPC çš„ servicerï¼Œå¹¶åœ¨å…¶ä¸­è‡ªå®šä¹‰å®é™…çš„æ¨ç†é€»è¾‘ã€‚
# ç›´æ¥è°ƒç”¨ CosyVoice2(args.model_dir) åˆå§‹åŒ–æ¨¡å‹å¯¹è±¡ã€‚å¦‚æœæˆåŠŸåˆ™åœ¨æ—¥å¿—é‡Œæ‰“å°â€œmodel loaded successfullyâ€ï¼Œå¤±è´¥åˆ™æŠ¥é”™ã€‚
class CosyVoiceServiceImpl(cosyvoice_pb2_grpc.CosyVoiceServicer):
    def __init__(self, args):
        try:
            # åˆå§‹åŒ–æ¨¡å‹
            self.cosyvoice = CosyVoice2(args.model_dir,load_trt=True, fp16=True)
            logging.info('CosyVoice2 model loaded successfully')

            # âœ… åˆå§‹åŒ–å…¨å±€ prompt_audio
            local_prompt_path = os.path.join(ROOT_DIR, "prompt_audios", "Roise.wav")
            prompt_speech,sample_rate = torchaudio.load(local_prompt_path)
            self.global_prompt_audio = prompt_speech

            # âœ… åˆå§‹åŒ–å…¨å±€ prompt_text
            self.global_prompt_text = "å¡è¿ªå¤«æˆ‘è¿˜æ²¡æœ‰å¬è¿‡ï¼Œåæ­£å—æ–¹é‚£äº›åŸå¸‚æˆ‘éƒ½æ²¡æ€ä¹ˆç©è¿‡ï¼Œæˆ‘å»è¿‡æœ€å—çš„åœ°æ–¹å¤§æ¦‚å°±æ˜¯ä¼¦æ•¦äº†"

            logging.info('æœåŠ¡å™¨å·²åˆå§‹åŒ– prompt_audio å’Œ prompt_text')

        except Exception as e:
            logging.error(f'Failed to load CosyVoice2 model: {e}')
            raise RuntimeError('Failed to load CosyVoice2 model')

        
    def VoiceChange(self, request, context):
        """å¤„ç†å®¢æˆ·ç«¯ä¸Šä¼ çš„æ–° prompt_audio å’Œ prompt_text"""
        logging.info("ğŸš€ Received new voice change request")

        if not request.HasField("voice_change_request"):
            logging.error("âŒ é”™è¯¯: æ”¶åˆ°çš„è¯·æ±‚ä¸æ˜¯ voice_change_request ç±»å‹")
            context.set_code(grpc.StatusCode.INVALID_ARGUMENT)
            context.set_details("Request is not a voice_change_request")
            return

        # è®¿é—®åµŒå¥—çš„ voice_change_request
        voice_change_request = request.voice_change_request

        # è§£æéŸ³é¢‘æ•°æ®
        self.global_prompt_audio = torch.from_numpy(
            np.array(np.frombuffer(voice_change_request.prompt_audio, dtype=np.int16))
        ).unsqueeze(dim=0).float() / (2**15)
        logging.info("âœ… æœåŠ¡å™¨å·²æ›´æ–° prompt_audio")

        if not self.global_prompt_audio.numel():
            logging.warning("âš ï¸ è­¦å‘Š: æœåŠ¡å™¨æ”¶åˆ°çš„ prompt_audio ä¸ºç©ºï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨ç†å¤±è´¥ï¼")

        # è§£ææ–‡æœ¬æ•°æ®
        self.global_prompt_text = voice_change_request.prompt_text.strip()
        logging.info(f"âœ… æœåŠ¡å™¨å·²æ›´æ–° prompt_text: {repr(self.global_prompt_text)}")
        
        if not self.global_prompt_text:
            logging.warning("âš ï¸ è­¦å‘Š: æœåŠ¡å™¨æ”¶åˆ°çš„ prompt_text ä¸ºç©ºï¼Œå¯èƒ½ä¼šå¯¼è‡´æ¨ç†å¤±è´¥ï¼")

        # ç«‹å³è¿”å›ç¡®è®¤ä¿¡æ¯
        response = cosyvoice_pb2.Response()
        response.tts_audio = b""  # è¿™é‡Œä¸éœ€è¦è¿”å›éŸ³é¢‘
        yield response

# æœåŠ¡ç«¯æœ€é‡è¦çš„åœ°æ–¹ï¼šå¤„ç†æ¥è‡ª gRPC å®¢æˆ·ç«¯çš„æ¨ç†è¯·æ±‚ï¼Œå¹¶è¿”å›æ¨ç†ç»“æœã€‚
    def Inference(self, request, context):
        # è¯†åˆ« gRPC å®¢æˆ·ç«¯ä¼ æ¥çš„è¯·æ±‚ç±»å‹åˆ°åº•æ˜¯å“ªä¸€ç§
        # è¾“å‡ºæ–‡å­—ã€æ¨¡ä»¿ç›®æ ‡å£°éŸ³ã€æ¨¡ä»¿ç›®æ ‡å£°éŸ³æ–‡å­—
        tts_text = request.zero_shot_request.tts_text

        prompt_text = self.global_prompt_text
        prompt_speech_16k = self.global_prompt_audio
        

        if request.HasField('zero_shot_request'):
            logging.info('Received zero-shot inference request')
            model_output = self.cosyvoice.inference_zero_shot(
                tts_text,
                prompt_text,
                prompt_speech_16k
            )
        elif request.HasField('cross_lingual_request'):
            logging.info('Received cross-lingual inference request')
            model_output = self.cosyvoice.inference_cross_lingual(
                request.cross_lingual_request.tts_text, 
                prompt_speech_16k
            )
        elif request.HasField('instruct_request'):
            logging.info('Received instruct inference request')
            model_output = self.cosyvoice.inference_instruct2(
                request.instruct_request.tts_text,
                request.instruct_request.instruct_text,
                prompt_speech_16k
            )            
        else:
            logging.warning('Invalid request type')
            context.set_code(grpc.StatusCode.INVALID_ARGUMENT)
            context.set_details('Invalid request type')
            return
        
        logging.info('Sending inference response')
        # for i in model_output: ... yield response è¡¨æ˜æ¯æ¬¡ä»æ¨¡å‹çš„ model_output é‡Œå–ä¸€æ®µéŸ³é¢‘ï¼Œå°±å³æ—¶è¿”å›ç»™å®¢æˆ·ç«¯ä¸€æ®µ Responseã€‚å®¢æˆ·ç«¯ä¼šä»¥æµï¼ˆstreamï¼‰çš„å½¢å¼æ¥æ”¶å¤šæ®µæ•°æ®
        for i in model_output:
            response = cosyvoice_pb2.Response()
            response.tts_audio = (i['tts_speech'].numpy() * (2 ** 15)).astype(np.int16).tobytes()
            yield response


def main():
    # argparse.ArgumentParser()ï¼šç”¨äºè§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument('--port', type=int, default=50000)
    parser.add_argument('--max_conc', type=int, default=4)
    parser.add_argument('--model_dir', type=str, default='iic/CosyVoice2-0.5B', help='local path or modelscope repo id')
    # è§£æç”¨æˆ·è¾“å…¥çš„å‘½ä»¤è¡Œå‚æ•°ï¼Œå¹¶å­˜å…¥ args å˜é‡
    args = parser.parse_args()

    # åˆ›å»º gRPC æœåŠ¡å™¨
    grpcServer = grpc.server(futures.ThreadPoolExecutor(max_workers=args.max_conc), maximum_concurrent_rpcs=args.max_conc)
    # åˆ›å»º CosyVoiceServiceImpl å®ä¾‹,å®ƒæ˜¯ gRPC çš„ servicerï¼Œè´Ÿè´£å¤„ç†å®¢æˆ·ç«¯çš„è¯·æ±‚
    cosyvoice_pb2_grpc.add_CosyVoiceServicer_to_server(CosyVoiceServiceImpl(args), grpcServer)
    # æœåŠ¡å™¨ç›‘å¬æ‰€æœ‰å¯ç”¨çš„ç½‘ç»œæ¥å£ï¼ˆå³å…è®¸å¤–éƒ¨è®¾å¤‡è®¿é—®ï¼‰
    grpcServer.add_insecure_port('0.0.0.0:{}'.format(args.port))
    grpcServer.start()
    logging.info("Server listening on 0.0.0.0:{}".format(args.port))
    grpcServer.wait_for_termination()


if __name__ == '__main__':
    main()
